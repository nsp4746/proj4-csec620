{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Nikhil Patil\n",
    "# CSEC 620 Project 4\n",
    "# NLP and SMS Spam \n",
    "\n",
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, warnings, nltk, random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "sms = pd.read_csv('SMSSpamCollection', sep=\"\\t\", names=['label', 'text'])\n",
    "print(\"\\n\\n\",sms.head(), end='\\n\\n')\n",
    "print(sms.info(), end='\\n\\n')\n",
    "print(sms.describe(), end='\\n\\n')\n",
    "print(\"The number of ham (non-spam) messages present are:\",sms['label'].value_counts()[0])\n",
    "print(\"The number of spam messages present are:\",sms['label'].value_counts()[1])"
   ],
   "id": "fa107a5fc7e6e1c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    Simple text tokenizer that is used with the Term Frequency Inverse Document Frequency Vectorizer.\\n \n",
    "    The way this function works is that it takes stop words from the Natural Language Toolkit NTLK[https://www.nltk.org/] \n",
    "    Passes them into a regex and uses WordNetLemmatizer to tokenize the text.\n",
    "    :param text: to be tokenized\n",
    "    :return: text tokens\n",
    "    '''    \n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    return tokens"
   ],
   "id": "da65fc2f4c299854",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_words = TfidfVectorizer(tokenizer=tokenize)",
   "id": "e71e4413f139061a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "spam_word_counts = all_words.fit_transform(sms.loc[sms['label']=='spam']['text'].values);\n",
    "\n",
    "spam_words = all_words.vocabulary_\n",
    "\n",
    "spam_words_df = pd.DataFrame.from_dict(spam_words, orient='index')\n",
    "spam_words_df.reset_index(inplace=True)\n",
    "spam_words_df.columns = ['word', 'counts']\n",
    "\n",
    "spam_words_df.sort_values(by='counts', ascending=False, inplace=True)\n",
    "spam_words_df.head(15)\n",
    "\n",
    "ham_word_counts = all_words.fit_transform(sms.loc[sms['label']=='ham']['text'].values)\n",
    "\n",
    "ham_words = all_words.vocabulary_\n",
    "\n",
    "ham_words_df = pd.DataFrame.from_dict(ham_words, orient='index')\n",
    "ham_words_df.reset_index(inplace=True)\n",
    "ham_words_df.columns = ['word', 'counts']\n",
    "\n",
    "ham_words_df.sort_values(by='counts', ascending=False, inplace=True)\n",
    "ham_words_df.head(15)"
   ],
   "id": "a308e29afffc8827",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tfidf = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "spam_word_weights = tfidf.fit_transform(spam_word_counts)\n",
    "top_idf_indices = tfidf.idf_.argsort()[:-5:-1]\n",
    "ind_to_word = all_words.get_feature_names_out()\n",
    "print('The most significant words for spam messages are:')\n",
    "for ind in top_idf_indices:\n",
    "    print(tfidf.idf_[ind], ind_to_word[ind])\n",
    "    \n",
    "ham_word_weights = tfidf.fit_transform(ham_word_counts)\n",
    "top_idf_indices = tfidf.idf_.argsort()[:-5:-1]\n",
    "ind_to_word = all_words.get_feature_names_out()\n",
    "print('The most significant words for ham messages are:')\n",
    "for ind in top_idf_indices:\n",
    "    print(tfidf.idf_[ind], ind_to_word[ind])"
   ],
   "id": "2581df6dc935c474",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class KMeans:\n",
    "    \"\"\"\n",
    "    A simple K-means clustering implementation.\n",
    "\n",
    "    This class implements the K-means clustering algorithm to group data points into `k` clusters\n",
    "    based on their distance to the cluster centroids. The centroids are initialized randomly, and\n",
    "    the algorithm iterates to minimize the within-cluster variance.\n",
    "\n",
    "    Attributes:\n",
    "        k (int): The number of clusters to form.\n",
    "        max_iters (int): The maximum number of iterations for the algorithm to converge. Default is 100.\n",
    "        centroids (np.ndarray): The coordinates of the cluster centroids.\n",
    "        clusters (np.ndarray): The cluster labels for each data point.\n",
    "    \n",
    "    Methods:\n",
    "        euclidean_distance(point1, point2): Computes the Euclidean distance between two data points.\n",
    "        fit(X): Fits the K-means model to the data by determining the cluster centroids and assigning data points to clusters.\n",
    "        predict(X): Predicts the nearest cluster for a new set of data points based on the fitted centroids.\n",
    "        create_clusters(X): Assigns data points to the nearest cluster based on the current centroids.\n",
    "        calculate_centroids(X): Calculates the new centroids based on the data points in each cluster.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=2, max_iters=100):\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        self.centroids = None\n",
    "        self.clusters = None\n",
    "\n",
    "    def euclidean_distance(self, point1, point2):\n",
    "        return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "\n",
    "    def fit(self, X):\n",
    "\n",
    "        np.random.seed(88) \n",
    "        random_indices = np.random.permutation(X.shape[0])[:self.k]\n",
    "        self.centroids = X[random_indices]\n",
    "\n",
    "        for i in range(self.max_iters):\n",
    "            self.clusters = self.create_clusters(X)\n",
    "            old_centroids = self.centroids\n",
    "            self.centroids = self.calculate_centroids(X)\n",
    "            if np.all(old_centroids == self.centroids):\n",
    "                break\n",
    "\n",
    "    def create_clusters(self, X):\n",
    "        clusters = [[] for _ in range(self.k)]\n",
    "        for idx, point in enumerate(X):\n",
    "            closest_centroid = np.argmin([self.euclidean_distance(point, centroid) for centroid in self.centroids])\n",
    "            clusters[closest_centroid].append(idx)\n",
    "        return clusters\n",
    "\n",
    "    def calculate_centroids(self, X):\n",
    "        centroids = np.zeros((self.k, X.shape[1]))\n",
    "        for cluster_idx, cluster in enumerate(self.clusters):\n",
    "            if cluster:  # Avoid empty clusters\n",
    "                centroids[cluster_idx] = np.mean(X[cluster], axis=0)\n",
    "        return centroids\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([np.argmin([self.euclidean_distance(x, centroid) for centroid in self.centroids]) for x in X])"
   ],
   "id": "62b4a79645f3618a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class KNearestNeighbors:\n",
    "    '''\n",
    "    K-Nearest Neighbors Implementation\n",
    "\n",
    "    Parameters: \n",
    "    k (int) : Number of components \n",
    "\n",
    "    Methods:\n",
    "    fit(X, y): Fit the model using X as training data and y as target values\n",
    "    predict(X): Predict the class labels for the provided data\n",
    "    _predict(x): Predict the class label for a single data point\n",
    "\n",
    "    '''\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def _predict(self, x):\n",
    "        distances = np.linalg.norm(self.X_train - x, axis=1)\n",
    "        \n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        \n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]"
   ],
   "id": "ee56aa980241926b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# data preprocessing \n",
    "\n",
    "word_counts = all_words.fit_transform(sms['text'].values)\n",
    "\n",
    "sms['label'] = sms['label'].map({'spam': 1, 'ham': 0})\n",
    "y = sms['label'].values\n",
    "\n",
    "tfidf_weights = tfidf.fit_transform(word_counts)\n",
    "\n",
    "X = tfidf_weights.toarray()"
   ],
   "id": "543d46208a829fda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# K Nearest Neighbors \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)\n",
    "\n",
    "\n",
    "knn = KNearestNeighbors(k=3)\n",
    "knn.fit(X_train, y_encoded)\n",
    "\n",
    "predictions = knn.predict(X_test)\n",
    "\n",
    "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
    "print(f\"KNN Model Accuracy:{accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"Classification Report for K-Nearest Neighbors (KNN):\")\n",
    "print(classification_report(y_test_encoded, predictions, target_names=['ham', 'spam']))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, index=['Actual Ham', 'Actual Spam'], columns=['Predicted Ham', 'Predicted Spam'])\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_df)"
   ],
   "id": "32099ede2b8d30b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# K-means \n",
    "k = 4\n",
    "kmeans = KMeans(k=k)\n",
    "kmeans.fit(X)\n",
    "\n",
    "predicted_clusters = kmeans.predict(X)\n",
    "\n",
    "cluster_to_label = {}\n",
    "\n",
    "for cluster in range(k):\n",
    "    cluster_indices = np.where(predicted_clusters == cluster)[0]\n",
    "    \n",
    "    if len(cluster_indices) > 0: \n",
    "        most_common_label = np.bincount(y[cluster_indices]).argmax()\n",
    "        cluster_to_label[cluster] = most_common_label\n",
    "\n",
    "mapped_predictions = np.array([cluster_to_label[cluster] for cluster in predicted_clusters])\n",
    "accuracy = accuracy_score(y, mapped_predictions)\n",
    "print(f\"K-means Clustering Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report for K-Means:\")\n",
    "print(classification_report(y, mapped_predictions, target_names=['ham', 'spam']))\n",
    "conf_matrix = confusion_matrix(y, predicted_clusters)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Cluster 0', 'Cluster 1','Cluster 2','Cluster 3'], yticklabels=['Actual Ham', 'Actual Spam'])\n",
    "plt.xlabel(\"Predicted Clusters\")\n",
    "plt.ylabel(\"Actual Labels\")\n",
    "plt.title(\"Confusion Matrix for K-means Clustering\")\n",
    "plt.show()"
   ],
   "id": "2014667156a836ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6deeb55d201c9fd5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
